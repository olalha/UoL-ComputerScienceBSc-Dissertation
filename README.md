# Rule-Based Synthetic Text Dataset Generator

## Introduction

This project provides a framework for generating synthetic text datasets, specifically designed to resemble user reviews or similar content, based on detailed, user-defined rules. The primary goal is to create datasets with precisely controlled characteristics for applications like Natural Language Processing (NLP) model training and evaluation.

*   **What it does:** It takes a structured "Rulebook" (typically defined in an Excel file) specifying desired dataset properties like topics, sentiments, word counts, and overall structure.
*   **How it works:** The system partitions these requirements into smaller "chunks," each representing a specific topic and sentiment. It then intelligently arranges these chunks into "collections" (e.g., individual reviews) using optimization algorithms (greedy approach followed by simulated annealing) to match the target distributions defined in the rulebook. Finally, it leverages a Large Language Model (LLM), currently configured for OpenAI, to generate text for each chunk and optionally merge them into cohesive collections.
*   **Positioning:** It offers fine-grained control over synthetic dataset generation, addressing the need for controlled data where real-world data might be scarce, imbalanced, or lack specific required attributes.
*   **Capabilities:** Key features include rule-based control over content and structure, a modular pipeline architecture, and an interactive Graphical User Interface (GUI) built with Streamlit.
*   **Limitations:** This tool is designed for generating synthetic data based on the defined chunk/collection structure. It is *not* intended for generating highly complex narratives or mimicking the full nuance of real-world human writing beyond the specified rules. It serves as a baseline platform that can be extended for more sophisticated generation tasks.

### Demo Video

A short demonstration video showcasing the application's workflow and features is available in the repository. 
Watching this video can provide a quick overview before setting up and running the application yourself.

## Key Terminology

*   **Rulebook:** The input specification defining the desired dataset characteristics. It's typically created using an Excel template (parsed by [`input_manager/rulebook_parser.py`](c:\Code\individual-project-olalha\input_manager\rulebook_parser.py)) and represented internally as a JSON object. It dictates the overall size, content title, collection mode (word/chunk count), topics, sentiment proportions per topic, chunk size constraints, and target distribution for collection sizes (`collection_ranges`). These are stored in two seperate sheets within the Excel workbook. Templates are availible within the repository.
*   **Chunk:** The smallest atomic unit of text generated. Each chunk focuses on a single topic and sentiment, adhering to a specific word count range defined in the rulebook's content rules.
*   **Collection:** A grouping of one or more chunks, designed to represent a single coherent text item (e.g., one product review). The `chunk_manager` forms collections by combining chunks to meet the overall topic, sentiment, and size distribution targets specified in the rulebook's `collection_ranges`.
*   **Dataset Structure:** An intermediate representation created by the `chunk_manager`. It defines the composition of the dataset in terms of collections and their constituent chunks (topic, sentiment, word count) *before* any text is generated by the LLM.

## Features

*   **Rulebook-Driven Specification:** Define dataset requirements using an Excel template, parsed into an internal JSON format.
*   **Granular Control:** Specify distributions for topics, sentiments (positive, neutral, negative), word counts per chunk, and collection sizes.
*   **Modular Pipeline:** Clear separation of concerns: Input Parsing -> Chunking & Optimization -> Text Generation -> Analysis.
*   **Chunk Partitioning:** Divides overall requirements into discrete chunks based on rulebook constraints ([`chunk_manager/chunk_partitioner.py`](c:\Code\individual-project-olalha\chunk_manager\chunk_partitioner.py)).
*   **Optimized Collection Forming:** Uses a greedy algorithm for initial chunk allocation followed by Simulated Annealing to optimize the arrangement of chunks into collections, aiming to match target size distributions ([`chunk_manager/dataset_handler.py`](c:\Code\individual-project-olalha\chunk_manager\dataset_handler.py)).
*   **LLM Integration:** Generates text using LLM APIs (currently OpenAI via [`generation_manager/api_handler.py`](c:\Code\individual-project-olalha\generation_manager\api_handler.py)).
*   **Flexible Prompting:** Utilizes Jinja2 for templating LLM prompts, allowing easy modification of generation instructions ([`generation_manager/prompt_builder.py`](c:\Code\individual-project-olalha\generation_manager\prompt_builder.py), `generation_manager/prompts/`). Supports different prompt strategies (e.g., generate chunks then merge, generate full collection).
*   **Parallel API Calls:** Efficiently generates text by making parallel requests to the LLM API ([`generation_manager/api_handler.py`](c:\Code\individual-project-olalha\generation_manager\api_handler.py)).
*   **Dataset Analysis & Visualization:** Includes tools to analyze the generated dataset's characteristics (distributions, counts) and visualize them ([`analysis_manager/`](c:\Code\individual-project-olalha\analysis_manager\)).
*   **Interactive GUI:** A Streamlit application ([`app.py`](c:\Code\individual-project-olalha\app.py), [`views/`](c:\Code\individual-project-olalha\views\)) allows users to upload rulebooks, generate dataset structures, trigger text generation, view results, and manage files.

## Pipeline Overview

1.  **Input Manager (`input_manager/`):**
    *   Parses rulebooks from Excel files, validating their structure and values ([`input_manager/rulebook_parser.py`](c:\Code\individual-project-olalha\input_manager\rulebook_parser.py)).
    *   Can also generate rulebooks programmatically for testing purposes ([`input_manager/rulebook_generator.py`](c:\Code\individual-project-olalha\input_manager\rulebook_generator.py)).
    *   Saves validated rulebooks as JSON files.

2.  **Chunk Manager (`chunk_manager/`):**
    *   Takes a validated rulebook as input.
    *   Partitions the total requirements defined in the rulebook into individual chunks (topic, sentiment, word count) ([`chunk_manager/chunk_partitioner.py`](c:\Code\individual-project-olalha\chunk_manager\chunk_partitioner.py)).
    *   Forms the `Dataset Structure`: Allocates these chunks into collections using a greedy algorithm for an initial solution.
    *   Optimizes the chunk allocation using Simulated Annealing to better match the target collection size distributions specified in the rulebook ([`chunk_manager/dataset_handler.py`](c:\Code\individual-project-olalha\chunk_manager\dataset_handler.py)).
    *   Outputs the dataset structure (collections with chunk definitions) as a JSON file.

3.  **Generation Manager (`generation_manager/`):**
    *   Takes the dataset structure and prompt templates as input.
    *   Renders specific prompts for the LLM using Jinja2 templates based on chunk details or collection requirements ([`generation_manager/prompt_builder.py`](c:\Code\individual-project-olalha\generation_manager\prompt_builder.py), `generation_manager/prompts/`).
    *   Manages interaction with the configured LLM API (e.g., OpenAI), handling requests, responses, rate limits, and retries ([`generation_manager/api_handler.py`](c:\Code\individual-project-olalha\generation_manager\api_handler.py)). Supports parallel API calls for efficiency.
    *   Implements different generation strategies:
        *   Multi-prompt: Generate text for each chunk individually, then merge chunks into a collection ([`generation_manager/text_generator.py`](c:\Code\individual-project-olalha\generation_manager\text_generator.py) - `generate_collection_texts_multi_prompt`). Uses `usr_chunk_gen.html` and `usr_merge_gen.html` prompts.
        *   Single-prompt: Generate the entire collection text in one go based on chunk requirements ([`generation_manager/text_generator.py`](c:\Code\individual-project-olalha\generation_manager\text_generator.py) - `generate_collection_texts_single_prompt`). Uses `usr_collection_gen.html` prompt.
    *   Updates the dataset structure JSON file with the generated text.

4.  **Analysis Manager (`analysis_manager/`):**
    *   Provides functions to calculate metrics and statistics about the generated dataset (e.g., word counts, topic/sentiment distributions) ([`analysis_manager/dataset_analyser.py`](c:\Code\individual-project-olalha\analysis_manager\dataset_analyser.py)).
    *   Offers functions to create plots and visualizations (e.g., distribution plots, pie charts) of the dataset characteristics ([`analysis_manager/dataset_visualizer.py`](c:\Code\individual-project-olalha\analysis_manager\dataset_visualizer.py)).

5.  **Views (`views/`, `view_components/`):**
    *   Provides the Streamlit-based user interface ([`app.py`](c:\Code\individual-project-olalha\app.py)).
    *   Allows users to upload/manage rulebooks and datasets ([`view_components/file_loader.py`](c:\Code\individual-project-olalha\view_components\file_loader.py), [`view_components/item_selector.py`](c:\Code\individual-project-olalha\view_components\item_selector.py)).
    *   Triggers the dataset structure generation and text generation processes.
    *   Displays rulebook details, dataset metrics, visualizations, and generated text ([`views/rulebook_page.py`](c:\Code\individual-project-olalha\views\rulebook_page.py), [`views/dataset_page.py`](c:\Code\individual-project-olalha\views\dataset_page.py)).

## Project Structure

```
.
├── _config/              # Configuration files (settings.yaml)
├── _data/                # Default storage for JSON rulebooks and datasets
│   ├── datasets/json/    # (Created automatically during usage)
│   └── rulebooks/json/   # (Created automatically during usage)
├── _templates/           # Location of Excel rulebook templates
├── analysis_manager/     # Dataset analysis and visualization code
├── chunk_manager/        # Chunk partitioning and collection forming logic
├── generation_manager/   # LLM interaction, prompt building, text generation
│   └── prompts/          # Jinja2 prompt templates (.html)
├── input_manager/        # Rulebook parsing, validation, and generation
├── testing/              # Automated tests (pytest)
├── utils/                # Shared utility functions (e.g., settings manager)
├── view_components/      # Reusable Streamlit UI components
├── views/                # Streamlit page definitions
├── .env                  # Environment variables (API keys - user-created)
├── .gitignore            # Git ignore rules
├── app.py                # Main Streamlit application entry point
├── FINAL-DEMO.mp4        # A short video illustrating the functionality
├── pytest.ini            # Pytest configuration
├── README.md             # This file
└── requirements.txt      # Python package dependencies
```

## Installation & Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd individual-project-olalha
    ```
2.  **Create and activate a Python virtual environment:**
    *   On macOS/Linux:
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        ```
    *   On Windows:
        ```bash
        python -m venv venv
        venv\Scripts\activate
        ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Create Environment File:**
    *   Create a file named `.env` in the project root directory (`c:\Code\individual-project-olalha`).
5.  **Configure API Key:**
    *   Add your OpenAI API key to the `.env` file. The file content should look like this:
        ```
        OPENAI_API_KEY='your_api_key_here'
        ```
    *   Replace `'your_api_key_here'` with your actual OpenAI API key.

## Configuration

*   Operational parameters like file paths, OpenAI API settings (retries, timeouts, rate limits), model names (e.g., `GPT4o`, `GPT4o-mini`), Excel parsing mappings, and algorithm parameters (Simulated Annealing, Greedy) can be adjusted in `_config/settings.yaml`.
*   The logic for loading and accessing these settings is handled by [`utils/settings_manager.py`](c:\Code\individual-project-olalha\utils\settings_manager.py).
*   The **OpenAI API key is mandatory** and must be set in the `.env` file in the project root directory as described in the Setup section.
*   If you need to change the required environment variables (e.g., add or remove API keys), the checks and loading logic in [`app.py`](c:\Code\individual-project-olalha\app.py) must be modified accordingly.

## Running the Application

1.  Ensure your virtual environment is activated.
2.  Navigate to the project root directory (`c:\Code\individual-project-olalha`).
3.  Run the Streamlit application:
    ```bash
    streamlit run app.py
    ```
4.  The application should open in your default web browser.

*Note: Deploying this application (e.g., using Streamlit Community Cloud) requires additional configuration specific to the deployment platform.*